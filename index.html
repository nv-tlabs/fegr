
<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<script src="http://www.google.com/jsapi" type="text/javascript"></script>
<script type="text/javascript">google.load("jquery", "1.3.2");</script>

<style type="text/css">
body {
/*    font-family: "Lato", "Titillium Web", "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;*/
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-weight: 300;
    font-size: 17px;
    margin-left: auto;
    margin-right: auto;
    width: 900px;
}
h1 {
    font-weight: 300;
    line-height: 1.15em;
}

h2 {
    font-size: 1.5em;
}
a:link,a:visited {
    color: #1772d0;
    text-decoration: none;
}
a:hover {
    color: #f09228;
}
h1, h2, h3 {
    text-align: center;
}
h1 {
    font-size: 36px;
    font-weight: 500;
}
h2 {
    font-weight: 400;
    margin: 16px 0px 4px 0px;
}
.paper-title {
    padding: 16px 0px 16px 0px;
}
section {
    margin: 32px 0px 32px 0px;
    text-align: justify;
    clear: both;
}
.col-6 {
     width: 16.6%;
     float: left;
}
.col-5 {
     width: 20%;
     float: left;
}
.col-4 {
     width: 25%;
     float: left;
}
.col-3 {
     width: 33%;
     float: left;
}
.col-2 {
     width: 50%;
     float: left;
}
.row, .author-row, .affil-row {
     overflow: auto;
}
.author-row, .affil-row {
    font-size: 18px;
    font-weight: 400;
}
.row {
    margin: 16px 0px 16px 0px;
}
.authors {
    font-size: 18px;
}
.affil-row {
    margin-top: 16px;
}
.teaser {
    max-width: 100%;
}
.text-center {
    text-align: center;  
}
.screenshot {
    width: 256px;
    border: 1px solid #ddd;
}
.screenshot-el {
    margin-bottom: 16px;
}
hr {
    height: 1px;
    border: 0; 
    border-top: 1px solid #ddd;
    margin: 0;
}
.material-icons {
    vertical-align: -6px;
}
p {
    line-height: 1.25em;
}
.caption {
    font-size: 16px;
    /*font-style: italic;*/
    color: #666;
    text-align: left;
    margin-top: 8px;
    margin-bottom: 8px;
}
video {
    display: block;
    margin: auto;
}
figure {
    display: block;
    margin: auto;
    margin-top: 10px;
    margin-bottom: 10px;
}
#bibtex pre {
    font-size: 14px;
    background-color: #eee;
    padding: 16px;
}
.blue {
    color: #2c82c9;
    font-weight: bold;
}
.orange {
    color: #d35400;
    font-weight: bold;
}
.flex-row {
    display: flex;
    flex-flow: row wrap;
    justify-content: space-around;
    padding: 0;
    margin: 0;
    list-style: none;
}

.paper-btn-parent {
    display: flex;
    justify-content: center;
    margin: 16px 0px;
}
.paper-btn {
  position: relative;
  text-align: center;

  display: inline-block;
  margin: 8px;
  padding: 8px 8px;

  border-width: 0;
  outline: none;
  border-radius: 2px;
  
  background-color: #1367a7;
  color: #ecf0f1 !important;
  font-size: 20px;
  width: 100px;
  font-weight: 600;
}
.paper-btn:hover {
    opacity: 0.85;
}

/*.supp-btn {
  position: relative;
  text-align: center;

  display: inline-block;
  margin: 8px;
  padding: 8px 8px;

  border-width: 0;
  outline: none;
  border-radius: 2px;
  
  background-color: #1367a7;
  color: #ecf0f1 !important;
  font-size: 20px;
  width: 150px;
  font-weight: 600;
}
.supp-btn:hover {
    opacity: 0.85;
}*/

.supp-btn {
  background-color: #fff;
  border: 1.5px solid #000;
  color: #000 !important;
  font-size: 16px;
  padding: 12px 24px;
  border-radius: 50px;
  text-align: center;
  text-decoration: none;
  display: inline-block;
  transition-duration: 0.4s;
  cursor: pointer;
  letter-spacing: 1px;
  margin: 8px;
  padding: 8px 8px;
  width: 150px;
  font-weight: 400;
}
.supp-btn:hover {
  background-color: #000;
  color: #fff !important;
}


.container {
    margin-left: auto;
    margin-right: auto;
    padding-left: 16px;
    padding-right: 16px;
}
.venue {
/*    color: #1367a7;*/
    color: #111;
    font-weight: 400;
}


.topnav {
  overflow: hidden;
/*  background-color: #EEEEEE;*/
}
.topnav a {
  float: left;
  color: black;
  text-align: center;
  padding: 14px 16px;
  text-decoration: none;
  font-size: 18px;
}

.centered {
  display: block;
  margin-left: auto;
  margin-right: auto;
}

.one {
  position: relative;
}
.two {
  position: absolute;
  transition: opacity .2s ease-in-out;
  -moz-transition: opacity .2s ease-in-out;
  -webkit-transition: opacity .2s ease-in-out;
}

.vertical-text {
  writing-mode: vertical-rl; /* vertical right-to-left */
  transform: rotate(180deg); /* optional: flip the text to make it read from top to bottom */
}

</style>


<div class="topnav" id="myTopnav">
  <a href="https://www.nvidia.com/"><img width="100%" src="assets/nvidia.svg"></a>
  <a href="https://nv-tlabs.github.io/" ><strong>Toronto AI Lab</strong></a>
</div>


<script type="text/javascript" src="../js/hidebib.js"></script>
<link href='https://fonts.googleapis.com/css?family=Titillium+Web:400,600,400italic,600italic,300,300italic' rel='stylesheet' type='text/css'>
<link href='https://fonts.googleapis.com/css?family=Lato:300,400,500,600,700,300italic,400italic,500italic,600italic,700italic' rel='stylesheet' type='text/css'>
<head>
    <title>Neural Fields meet Explicit Geometric Representations (FEGR)</title>
    <meta property="og:description" content="Neural Fields meet Explicit Geometric Representations (FEGR)"/>
    <link href="https://fonts.googleapis.com/css2?family=Material+Icons" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-141699104-1"></script>
    <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
    gtag('config', 'UA-141699104-1');
    </script>
</head>


<body>
<div class="container">
    <div class="paper-title">
        <h1>Neural Fields meet Explicit Geometric Representations for Inverse Rendering of Urban Scenes </h1>
    </div>
    
    <div id="authors">
        <div class="author-row">
            <div class="col-5 text-center"><a href="https://www.cs.toronto.edu/~zianwang/">Zian Wang</a><sup>1,2,3</sup></div>
            <div class="col-5 text-center"><a href="https://www.cs.toronto.edu/~shenti11/">Tianchang Shen</a><sup>1,2,3</sup></div>
            <div class="col-5 text-center"><a href="https://www.cs.toronto.edu/~jungao/">Jun Gao</a><sup>1,2,3</sup></div>
            <div class="col-5 text-center"><a href="https://shengyuh.github.io">Shengyu Huang</a><sup>1,4</sup></div>
            <div class="col-5 text-center"><a href="https://research.nvidia.com/person/jacob-munkberg">Jacob Munkberg</a><sup>1</sup></div>
            <div class="col-4 text-center"><a href="https://research.nvidia.com/person/jon-hasselgren">Jon Hasselgren</a><sup>1</sup></div>
            <div class="col-4 text-center"><a href="https://zgojcic.github.io">Zan Gojcic</a><sup>1</sup></div>
            <div class="col-4 text-center"><a href="https://www.cs.toronto.edu/~wenzheng/">Wenzheng Chen</a><sup>1,2,3</sup></div>
            <div class="col-4 text-center"><a href="https://www.cs.toronto.edu/~fidler/">Sanja Fidler</a><sup>1,2,3</sup></div>
        </div>

        <div class="affil-row">
            <div class="col-4 text-center"><sup>1</sup> NVIDIA</a></div>
            <div class="col-4 text-center"><sup>2</sup> University of Toronto</div>
            <div class="col-4 text-center"><sup>3</sup> Vector Institute</div>
            <div class="col-4 text-center"><sup>4</sup> ETH Zurich</div>
        </div>
        <div class="affil-row">
            <div class="venue text-center">CVPR 2023</div>
        </div>

        <div style="clear: both">
            <div class="paper-btn-parent">
            <a class="supp-btn" href="assets/fegr_paper.pdf">
                <span class="material-icons"> description </span> 
                 Paper
            </a>
            <a class="supp-btn" href="assets/fegr_supp.pdf">
                <span class="material-icons"> description </span> 
                 Supp
            </a>
            <a class="supp-btn" href="https://youtu.be/1KvHY3tlhhY">
                <span class="material-icons"> description </span> 
                 Video
            </a>
            <a class="supp-btn" href="assets/fegr_bib.txt">
                <span class="material-icons"> description </span> 
                 BibTeX
            </a>
            </div>
        </div>
    </div>

    <section id="teaser-videos">
        <div class="flex-row">
            <div class="vertical-text" style="width: 4%; display: flex; align-items: center; height: 150px; font-weight: bold;">
            Relighting
            </div>
            <figure style="width: 27%; float: left">
                <video class="centered" width="100%" controls muted loop autoplay>
                    <source src="assets/bmvs.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                </video>
            </figure>
            <figure style="width: 31.5%; float: left">
                <video class="centered" width="100%" controls muted loop autoplay>
                    <source src="assets/nerfosr.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                </video>
            </figure>
            <figure style="width: 36%; float: left">
                <video class="centered" width="100%" controls muted loop autoplay>
                    <source src="assets/driving.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                </video>
            </figure>
        </div>
        <div class="flex-row">
            <div class="vertical-text" style="width: 4%; display: flex; align-items: center; height: 175px; font-weight: bold;">
            Object Insertion
            </div>
            <figure style="width: 47.5%; float: center">
                <video class="centered" width="100%" controls muted loop autoplay>
                    <source src="assets/carclash_crop.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                </video>
            </figure>
            <figure style="width: 47.5%; float: center">
                <video class="centered" width="100%" controls muted loop autoplay>
                    <source src="assets/kid_crop.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                </video>
            </figure>
        </div>
        <p class="caption" style="margin-bottom: 3px;">
            <strong>FEGR enables Novel View Relighting and Virtual Object Insertion for a diverse range of scenes.</strong> 
            "Neural <strong>F</strong>ields meet <strong>E</strong>xplicit <strong>G</strong>eometric <strong>R</strong>epresentations", abbreviated as FEGR, is an approach for reconstructing scene geometry and recovering intrinsic properties of the scene from posed camera images. Our approach works both for single and multi-illumination captured data. FEGR enables various downstream applications such as VR and AR where users may want to control the lighting of the environment and insert desired 3D objects into the scene. 
        </p>
    </section>

    <section id="abstract"/>
        <h2>Abstract</h2>
        <hr>
        <p>Reconstruction and intrinsic decomposition of scenes from captured imagery would enable many applications such as relighting and virtual object insertion. Recent NeRF based methods achieve impressive fidelity of 3D reconstruction, but bake the lighting and shadows into the radiance field, while mesh-based methods that facilitate intrinsic decomposition through differentiable rendering have not yet scaled to the complexity and scale of outdoor scenes. We present a novel inverse rendering framework for large urban scenes capable of jointly reconstructing the scene geometry, spatially-varying materials, and HDR lighting from a set of posed RGB images with optional depth. Specifically, we use a neural field to account for the primary rays, and use an explicit mesh (reconstructed from the underlying neural field) for modeling secondary rays that produce higher-order lighting effects such as cast shadows. By faithfully disentangling complex geometry and materials from lighting effects, our method enables photorealistic relighting with specular and shadow effects on several outdoor datasets. Moreover, it supports physics-based scene manipulations such as virtual object insertion with ray-traced shadow casting. 
        </p>
    </section>

    <section id="intro">
        <h2>Video (2 minutes)</h2>
        <hr>
        <figure style="width: 100%;">
            <!-- <video class="centered" width="100%" controls>
                <source src="assets/FEGR_Narrated_Video_1080p.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video> -->
            <center>
                <iframe width="864" height="486"
                    src="https://www.youtube.com/embed/1KvHY3tlhhY">
                </iframe>
            </center>
        </figure>
        <p style="font-weight: 400; ">Combined with other NVIDIA technology, FEGR is one component of the <a href="https://www.youtube.com/watch?v=vgot-CK1xRk">Neural Reconstruction Engine</a> announced in <a href="https://youtu.be/PWcNlRI00jo?t=2473">GTC Sept 2022 Keynote</a>. </p>
    </section>

    <section id="method">
        <h2>Hybrid Rendering</h2>
        <hr>
        <p>
            <strong>Motivation. </strong> 
            Neural radiance fields (NeRFs) have recently emerged as a powerful neural reconstruction approach that enables photo-realistic novel-view synthesis. 
            To be compatible with modern graphics pipeline and support applications such as relighting and object insertion, recent works also build on top of neural fields and explore a full inverse rendering formulation. 
            However, due to the volumetric nature of the neural fields, it remains an open challenge for neural fields to represent higher order lighting effects such as cast shadows via ray-tracing. 
            In contrast to NeRF, explicit mesh representations are compatible with graphics pipeline and can effectively leverage classic graphics techniques for ray-tracing. While these methods demonstrate remarkable performance in a single-object setting, they are limited in resolution when scaling up to encompass larger scenes. 
            <br>
            <br>
            <strong>Overview. </strong> In this work, we combine the advantages of the neural fields and explicit mesh representations and propose FEGR, a new hybrid rendering pipeline for inverse rendering of large urban scenes. 
            Specifically, we first use the neural field to perform volumetric rendering of primary rays into a G-buffer that includes the surface normal, base color, and material parameters for each pixel. We then extract the mesh from the underlying signed distance field, and perform the shading pass in which we compute illumination by integrating over the hemisphere at the shading point using Monte Carlo ray tracing. 
            <br>
            <br>
            The two-step hybrid rendering corresponds to the two passes in deferred shading, alleviating the rendering cost by leveraging mesh-based ray-tracing, while maximally preserves the high-fidelity rendering of the volumetric neural field. 
        </p>
        <figure style="width: 100%;">
            <img width="100%" src="assets/model.jpg">
            <p class="caption" style="margin-bottom: 3px;">
        </figure>
        <p>
        </p>
    </section>

    <section id="results1">
        <h2>Content Digitization</h2>
        <hr>
        <p>
            FEGR decomposes the scene into multiple-passes of physics-based material properties. Assets reconstructed with FEGR are compatible with modern graphics pipelines and can be exported as 3D file formats (such as gltf and USD) and loaded into graphics engines for further editing. 
        </p>
        <figure style="width: 100%;">
            <video class="centered" width="100%" controls muted loop autoplay>
                <source src="assets/FEGR_asset_clip.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
        </figure>
    </section>

    <section id="results2">
        <h2>Relighting</h2>
        <hr>
        <p>
            FEGR can be used to reconstruct the challenging scenes from autonomous driving. The reconstructed sequences can be relighted with diverse lighting conditions, generating an abundance of training and testing data for the perception models. 
        </p>
        <figure style="width: 100%;">
            <video class="centered" width="100%" controls muted loop autoplay>
                <source src="assets/FEGR_relighting_clip.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
        </figure>
    </section>

    <section id="results3">
        <h2>Virtual Object Insertion </h2>
        <hr>
        <p>
            Reconstructed scenes can also be populated with synthetic or AI generated objects where physics can be applied. 
            Photorealistic object insertion into the reconstructed scenes provides a way to generate rarely observed but safety critical scenarios. 
        </p>
        <figure style="width: 100%;">
            <video class="centered" width="100%" controls muted loop autoplay>
                <source src="assets/FEGR_AR_clip.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
        </figure>
    </section>

    <section id="bibtex">
        <h2>Citation</h2>
        <hr>
<pre><code>@inproceedings{wang2023fegr,
title = {Neural Fields meet Explicit Geometric Representations for Inverse Rendering of Urban Scenes}, 
author = {Zian Wang and Tianchang Shen and Jun Gao and Shengyu Huang and Jacob Munkberg 
and Jon Hasselgren and Zan Gojcic and Wenzheng Chen and Sanja Fidler},
booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2023}
}
</code></pre>
    </section>

    
    <section id="paper">
        <h2>Paper</h2>
        <hr>
        <div class="flex-row">
            <div style="box-sizing: border-box; padding: 16px; margin: auto;">
                <a href="assets/fegr_paper.pdf"><img class="screenshot" src="assets/paper_preview.jpg"></a>
            </div>
            <div style="width: 60%">
                <p><b>Neural Fields meet Explicit Geometric Representations for Inverse Rendering of Urban Scenes</b></p>
                <p>Zian Wang, Tianchang Shen, Jun Gao, Shengyu Huang, Jacob Munkberg, Jon Hasselgren, Zan Gojcic, Wenzheng Chen, Sanja Fidler</p>
                <div><span class="material-icons"> description </span><a href="assets/fegr_paper.pdf"> Paper </a></div>
                <div><span class="material-icons"> description </span><a href="assets/fegr_supp.pdf"> Supp PDF </a></div>
                <div><span class="material-icons"> insert_comment </span><a href="assets/fegr_bib.txt"> BibTeX</a></div>
            </div>
        </div>
    </section>

    <section id="acknowledgment"/>
        <h2>Acknowledgment</h2>
        <hr>
        <p>The authors appreciate the support from <a href="https://jme.pub">Janick Martinez Esturo</a>, <a href="https://www.linkedin.com/in/evgeny-toropov-9bb14210b/">Evgeny Toropov</a>, <a href="https://www.linkedin.com/in/chen-chen-00499624/">Chen Chen</a> on the data processing pipeline, and the help from <a href="https://www.linkedin.com/in/linasong/">Lina Halper</a>, <a href="https://zhengyiluo.github.io">Zhengyi Luo</a>, <a href="https://www.linkedin.com/in/kelly-guo-18316982/?originalSubdomain=ca">Kelly Guo</a>, <a href="https://www.linkedin.com/in/gavstate/?originalSubdomain=ca">Gavriel State</a> on creating the edited scenes. 
        We would also like to thank the authors of <a href="https://4dqv.mpi-inf.mpg.de/NeRF-OSR/">NeRF-OSR</a> for discussion on dataset and experiment details. 
        </p>
    </section>

    <!-- End of page -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td>
          <br>
          <p align="middle">
            <font size="3">
            </font>
          </p>
        </td>
      </tr>
    </table>

</div>
</body>
</html>


